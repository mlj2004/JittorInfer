// Chat support (incl. tool call grammar constraining & output parsing) w/ generic & custom template handlers.

#pragma once

#include <string>
#include <vector>

#include "common_local.h"
#include "llama.h"

struct common_chat_local_templates;

struct common_chat_local_tool_call {
    std::string name;
    std::string arguments;
    std::string id;
};

struct common_chat_local_msg_content_part {
    std::string type;
    std::string text;
};

struct common_chat_local_msg {
    std::string                                     role;
    std::string                                     content;
    std::vector<common_chat_local_msg_content_part> content_parts;
    std::vector<common_chat_local_tool_call>        tool_calls;
    std::string                                     reasoning_content;
    std::string                                     tool_name;
    std::string                                     tool_call_id;
};

struct common_chat_local_templates_inputs {
    std::vector<common_chat_local_msg> messages;
    std::string                        grammar;
    std::string                        json_schema;
    bool                               add_generation_prompt = true;
    bool                               use_jinja             = true;
    // Parameters below only supported when use_jinja is true
    // std::vector<common_chat_local_tool> tools;
    // common_chat_local_tool_choice tool_choice = COMMON_CHAT_TOOL_CHOICE_AUTO;
    bool                               parallel_tool_calls   = false;
    bool                               extract_reasoning     = true;
};

enum common_chat_local_format {
    COMMON_CHAT_FORMAT_CONTENT_ONLY,
    COMMON_CHAT_FORMAT_GENERIC,
    COMMON_CHAT_FORMAT_MISTRAL_NEMO,
    COMMON_CHAT_FORMAT_LLAMA_3_X,
    COMMON_CHAT_FORMAT_LLAMA_3_X_WITH_BUILTIN_TOOLS,
    COMMON_CHAT_FORMAT_DEEPSEEK_R1,
    COMMON_CHAT_FORMAT_DEEPSEEK_R1_EXTRACT_REASONING,
    COMMON_CHAT_FORMAT_FIREFUNCTION_V2,
    COMMON_CHAT_FORMAT_FUNCTIONARY_V3_2,
    COMMON_CHAT_FORMAT_FUNCTIONARY_V3_1_LLAMA_3_1,
    COMMON_CHAT_FORMAT_HERMES_2_PRO,
    COMMON_CHAT_FORMAT_COMMAND_R7B,
    COMMON_CHAT_FORMAT_COMMAND_R7B_EXTRACT_REASONING,

    COMMON_CHAT_FORMAT_COUNT,  // Not a format, just the # formats
};

struct common_chat_local_params {
    common_chat_local_format                  format = COMMON_CHAT_FORMAT_CONTENT_ONLY;
    std::string                               prompt;
    std::string                               grammar;
    bool                                      grammar_lazy = false;
    std::vector<common_grammar_local_trigger> grammar_triggers;
    std::vector<std::string>                  preserved_tokens;
    std::vector<std::string>                  additional_stops;
};

void common_chat_local_templates_free(struct common_chat_local_templates * tmpls);

struct common_chat_local_templates_deleter {
    void operator()(common_chat_local_templates * tmpls) { common_chat_local_templates_free(tmpls); }
};

typedef std::unique_ptr<struct common_chat_local_templates, common_chat_local_templates_deleter>
    common_chat_local_templates_ptr;

common_chat_local_templates_ptr common_chat_local_templates_init(const struct llama_model * model);

struct common_chat_local_params common_chat_local_templates_apply(
    const struct common_chat_local_templates * tmpls, const struct common_chat_local_templates_inputs & inputs);

// Format single message, while taking into account the position of that message in chat history
std::string common_chat_local_format_single(const struct common_chat_local_templates * tmpls,
                                            const std::vector<common_chat_local_msg> & past_msg,
                                            const common_chat_local_msg & new_msg, bool add_ass, bool use_jinja);
